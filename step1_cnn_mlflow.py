# step1_cnn_mlflow.py
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import numpy as np
import mlflow
import mlflow.pytorch
import os

# 1. ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ Device: {device}")

# í´ë” ìë™ ìƒì„± (ì—¬ê¸°ê°€ ë„¤ ê±±ì •ì„ í•´ê²°í•´ì£¼ëŠ” ë¶€ë¶„!)
if not os.path.exists('results'): os.makedirs('results')
if not os.path.exists('data'): os.makedirs('data')

# 2. ë°ì´í„° ì¤€ë¹„
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# ë°ì´í„° ë‹¤ìš´ë¡œë“œ
print("ğŸ“¥ ë°ì´í„° í™•ì¸ ì¤‘...")
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# 3. ëª¨ë¸ ì •ì˜ (Simple CNN)
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) 
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.relu = nn.ReLU()
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(-1, 64 * 7 * 7)
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

def train_and_log(seed):
    run_name = f"CNN_Trial_Seed_{seed}"
    
    # MLflow ê¸°ë¡ ì‹œì‘ (ìë™ìœ¼ë¡œ mlruns í´ë”ê°€ ìƒê¹€!)
    with mlflow.start_run(run_name=run_name):
        params = {
            "model_type": "CNN",
            "seed": seed,
            "epochs": 3,  # ë¹ ë¥´ê²Œ ê²°ê³¼ ë³´ê¸° ìœ„í•´ 3 epoch
            "batch_size": 128,
            "learning_rate": 0.001
        }
        mlflow.log_params(params)
        
        torch.manual_seed(seed)
        train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)
        
        model = SimpleCNN().to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])
        
        # í•™ìŠµ
        model.train()
        for epoch in range(params['epochs']):
            for data, target in train_loader:
                data, target = data.to(device), target.to(device)
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
            
        # í‰ê°€
        model.eval()
        correct = 0
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
        
        acc = 100. * correct / len(test_loader.dataset)
        
        # ê²°ê³¼ ê¸°ë¡
        mlflow.log_metric("test_accuracy", acc)
        mlflow.pytorch.log_model(model, "model")
        
        print(f"Trial {seed+1}/10 (Seed {seed}) : Accuracy = {acc:.2f}%")
        return acc

if __name__ == "__main__":
    # ì‹¤í—˜ ì´ë¦„ ì„¤ì •
    mlflow.set_experiment("MNIST_Hypothesis_Testing")
    cnn_accuracies = []
    
    print("\nğŸ”¬ [MLflow] CNN Baseline 10íšŒ ë°˜ë³µ ì‹¤í—˜ ì‹œì‘...")
    for seed in range(10):
        acc = train_and_log(seed)
        cnn_accuracies.append(acc)
    
    # ê²°ê³¼ ì €ì¥ (ë‚˜ì¤‘ì— T-testìš©)
    np.save("results/cnn_accuracies.npy", np.array(cnn_accuracies))
    
    mean_acc = np.mean(cnn_accuracies)
    print("\n" + "="*40)
    print(f"âœ… í‰ê·  ì •í™•ë„: {mean_acc:.4f}%")
    print("="*40)